{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import optparse\n",
    "import sys\n",
    "import models\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "import math\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "#these two functions are found in https://u.cs.biu.ac.il/~yogo/courses/mt2013/ass3/utils.py, bar-llan university\n",
    "\n",
    "def bitmap(sequence):\n",
    "  \"\"\" Generate a coverage bitmap for a sequence of indexes \"\"\"\n",
    "  return reduce(lambda x,y: x|y, map(lambda i: int('1'+'0'*i,2), sequence), 0)\n",
    "\n",
    "\n",
    "def bitmap2str(b, n, on='o', off='.'):\n",
    "  \"\"\" Generate a length-n string representation of bitmap b \"\"\"\n",
    "  return '' if n==0 else (on if b&1==1 else off) + bitmap2str(b>>1, n-1, on, off)\n",
    "\n",
    "def create_new_hypothesis(h, phrase, lm, new_used_pos, j, f):\n",
    "    logprob = h.logprob + phrase.logprob\n",
    "    lm_state = h.lm_state\n",
    "    for word in phrase.english.split():\n",
    "        (lm_state, word_logprob) = lm.score(lm_state, word)\n",
    "        logprob += word_logprob\n",
    "    logprob += lm.end(lm_state) if j == len(f) else 0.0\n",
    "    return hypothesis(logprob, lm_state, h, phrase, new_used_pos, j), logprob\n",
    "\n",
    "def recombination(stacks, new_len, recomb_key, new_hypothesis, logprob):\n",
    "    if recomb_key not in stacks[new_len] or stacks[new_len][recomb_key].logprob < logprob:\n",
    "        stacks[new_len][recomb_key] = new_hypothesis\n",
    "    return stacks\n",
    "\n",
    "def generate_hypothesis(h, i, j, f, tm, lm, stacks, l, alpha, reorder_limit, opts):\n",
    "    if abs(i - h.prev_end_pos - 1) > reorder_limit or h.used_pos & bitmap(range(i, j)):\n",
    "        return stacks\n",
    "\n",
    "    if f[i:j] in tm:\n",
    "        new_used_pos = h.used_pos | bitmap(range(i, j))\n",
    "        len_p = j - i\n",
    "        for phrase in tm[f[i:j]]:\n",
    "            new_hypothesis, logprob = create_new_hypothesis(h, phrase, lm, new_used_pos, j, f)\n",
    "            new_len = l + len_p\n",
    "            recomb_key = (new_hypothesis.lm_state, new_used_pos, j)\n",
    "            stacks = recombination(stacks, new_len, recomb_key, new_hypothesis, logprob)\n",
    "    return stacks\n",
    "\n",
    "\n",
    "\n",
    "optparser = optparse.OptionParser()\n",
    "optparser.add_option(\"-i\", \"--input\", dest=\"input\", default=\"data/input\", help=\"File containing sentences to translate (default=data/input)\")\n",
    "optparser.add_option(\"-t\", \"--translation-model\", dest=\"tm\", default=\"data/tm\", help=\"File containing translation model (default=data/tm)\")\n",
    "optparser.add_option(\"-l\", \"--language-model\", dest=\"lm\", default=\"data/lm\", help=\"File containing ARPA-format language model (default=data/lm)\")\n",
    "optparser.add_option(\"-n\", \"--num_sentences\", dest=\"num_sents\", default=sys.maxsize, type=\"int\", help=\"Number of sentences to decode (default=no limit)\")\n",
    "optparser.add_option(\"-k\", \"--translations-per-phrase\", dest=\"k\", default=1, type=\"int\", help=\"Limit on number of translations to consider per phrase (default=1)\")\n",
    "optparser.add_option(\"-s\", \"--stack-size\", dest=\"s\", default=1, type=\"int\", help=\"Maximum stack size (default=1)\")\n",
    "optparser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", action=\"store_true\", default=False,  help=\"Verbose mode (default=off)\")\n",
    "optparser.add_option(\"-r\", \"--reordering-limit\", dest=\"rl\", default=sys.maxsize, type=int, help=\"Reordering limit (default=no limit\")\n",
    "opts = optparser.parse_args()[0]\n",
    "\n",
    "\n",
    "tm = models.TM(opts.tm, opts.k)\n",
    "lm = models.LM(opts.lm)\n",
    "french = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]\n",
    "reorder_limit = opts.rl\n",
    "alpha = 1.3\n",
    "\n",
    "# tm should translate unknown words as-is with probability 1\n",
    "for word in set(sum(french,())):\n",
    "  if (word,) not in tm:\n",
    "    tm[(word,)] = [models.phrase(word, 0.0)]\n",
    "\n",
    "sys.stderr.write(\"Decoding %s...\\n\" % (opts.input,))\n",
    "for f in french:\n",
    "  # The following code implements a monotone decoding\n",
    "  # algorithm (one that doesn't permute the target phrases).\n",
    "  # Hence all hypotheses in stacks[i] represent translations of \n",
    "  # the first i words of the input sentence. You should generalize\n",
    "  # this so that they can represent translations of *any* i words.\n",
    "  hypothesis = namedtuple(\"hypothesis\", \"logprob, lm_state, predecessor, phrase, used_pos, prev_end_pos\")\n",
    "  # used_pos: an integer bitmap indicating the position that is being used\n",
    "  # prev_end_pos: an integer representing the end position of last added phrase\n",
    "  initial_hypothesis = hypothesis(0.0, lm.begin(), None, None, 0, -1)\n",
    "  stacks = [{} for _ in f] + [{}]\n",
    "  stacks[0][(lm.begin(), 0, -1)] = initial_hypothesis\n",
    "  # stack l stores the hypotheses with l words translated\n",
    "  for l, stack in enumerate(stacks[:-1]):\n",
    "    best = max(stack.values(), key=lambda h: -h.logprob)\n",
    "    pruned = sorted(\n",
    "        filter(lambda h: h.logprob >= alpha*best.logprob, stack.values()), \n",
    "        key=lambda h: -h.logprob\n",
    "    )[:opts.s]\n",
    "\n",
    "    for h in pruned:\n",
    "        for i in range(len(f)):\n",
    "            for j in range(i + 1, len(f) + 1):\n",
    "                stacks = generate_hypothesis(h, i, j, f, tm, lm, stacks, l, alpha, reorder_limit, opts)\n",
    "\n",
    "\n",
    "  winner = max(stacks[-1].values(), key=lambda h: h.logprob)\n",
    "  def extract_english(h): \n",
    "    # return \"\" if h.predecessor is None else \"%s%s \" % (extract_english(h.predecessor), h.phrase.english)\n",
    "    results = []\n",
    "    while h.predecessor is not None:\n",
    "      results.append((h.phrase.english, h.prev_end_pos))\n",
    "      h = h.predecessor\n",
    "    results = sorted(results, key=lambda x:x[1])\n",
    "    words = list(zip(*results))[0]\n",
    "    return ' '.join(words)\n",
    "\n",
    "  print(extract_english(winner))\n",
    "\n",
    "  if opts.verbose:\n",
    "    def extract_tm_logprob(h):\n",
    "      return 0.0 if h.predecessor is None else h.phrase.logprob + extract_tm_logprob(h.predecessor)\n",
    "    tm_logprob = extract_tm_logprob(winner)\n",
    "    sys.stderr.write(\"LM = %f, TM = %f, Total = %f\\n\" % \n",
    "      (winner.logprob - tm_logprob, tm_logprob, winner.logprob))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
